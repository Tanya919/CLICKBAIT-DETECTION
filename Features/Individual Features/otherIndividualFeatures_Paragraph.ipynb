{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEg3rw99PiHX",
        "outputId": "ffde5cd4-1b56-4c62-e833-d59052bfa515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from statistics import variance\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "jx21-oZyQZ_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2srLoA7QfNV",
        "outputId": "48704b6c-f4a1-416c-b540-42b478d16605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kizoawf1QfR1",
        "outputId": "94e0a0be-c235-4940-8694-365c609b861c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aE9ZIYEQfXf",
        "outputId": "21c1e002-b64c-4bd4-ba71-ab4aaf97b1e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of n-gram POS tags for paragraphs."
      ],
      "metadata": {
        "id": "I5tPCM-ah1KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def noNNP(postText):\n",
        "  nnpCount = []\n",
        "  for row in range(len(postText)):\n",
        "    if type(postText[row])==float:\n",
        "      nnpCount.append(0)\n",
        "      continue\n",
        "    count = 0\n",
        "    for word,pos in nltk.pos_tag(nltk.word_tokenize(postText[row])):\n",
        "          if (pos == 'NNP'):\n",
        "            count += 1\n",
        "    nnpCount.append(count)\n",
        "  return nnpCount"
      ],
      "metadata": {
        "id": "FLODQg7HQfam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def noTokens(paragraphs):\n",
        "  tokenCount = []\n",
        "  for row in range(len(paragraphs)):\n",
        "    if type(paragraphs[row])==float:\n",
        "      tokenCount.append(0)\n",
        "      continue\n",
        "    tokens = nltk.word_tokenize(paragraphs[row])\n",
        "    tokenCount.append(len(tokens))\n",
        "    #print(tokens)\n",
        "  #print(tokenCount)\n",
        "  return tokenCount"
      ],
      "metadata": {
        "id": "ZxdhyDzLQfdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wordLen(paragraphs):\n",
        "  wordlen = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float or len(item) == 0):\n",
        "      wordlen.append(0)\n",
        "      continue\n",
        "    token = item.split()\n",
        "    wordlength = sum(len(word) for word in token)\n",
        "    #print(token)\n",
        "    #print(wordlength)\n",
        "    wordlen.append(len(wordlength))\n",
        "  return wordlen"
      ],
      "metadata": {
        "id": "-oYkPyrXQfhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_NNP(phrase):\n",
        "  cnt = 0\n",
        "  for item in phrase:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'NNP':\n",
        "      cnt += 1\n",
        "      #print(tagged)\n",
        "  return cnt\n",
        "\n",
        "#feature 5 - POS 2 GRAM NNP\n",
        "def NNP_NNPCount(paragraphs):\n",
        "  f = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item)== float):\n",
        "      f.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      #print(output)\n",
        "      cnt = NNP_NNP(output)\n",
        "      f.append(cnt)\n",
        "    else:\n",
        "      f.append(0)\n",
        "  return f"
      ],
      "metadata": {
        "id": "yq2l4sD1QfkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def startsWithNumber(paragraphs):\n",
        "  startNo = []\n",
        "  for row in range(len(paragraphs)):\n",
        "    if(type(paragraphs[row]) == float):\n",
        "      startNo.append(0)\n",
        "      continue\n",
        "    if(paragraphs[row][0].isdigit()):\n",
        "      startNo.append(1)\n",
        "    else:\n",
        "      startNo.append(0)\n",
        "  return startNo"
      ],
      "metadata": {
        "id": "tTMjSFFmQfnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def averageLen(paragraphs):\n",
        "  averageLenWords = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item)== float):\n",
        "      averageLenWords.append(0)\n",
        "      continue\n",
        "    words = item.split()\n",
        "    average = round(sum(len(word) for word in words) / len(words),2)\n",
        "    averageLenWords.append(average)\n",
        "  return averageLenWords"
      ],
      "metadata": {
        "id": "sHBmlZ1MQfrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def INCount(paragraphs):\n",
        "  INCount = []\n",
        "  for row in range(len(paragraphs)):\n",
        "    count = 0\n",
        "    if(type(paragraphs[row]) == float):\n",
        "      INCount.append(0)\n",
        "      continue \n",
        "    for word,pos in nltk.pos_tag(nltk.word_tokenize(paragraphs[row])):\n",
        "          if (pos == 'IN'):\n",
        "            count += 1\n",
        "    INCount.append(count)\n",
        "  return INCount"
      ],
      "metadata": {
        "id": "lb84zti6Qfwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_VBZ(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'VBZ':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def NNP_VBZCount(paragraphs):\n",
        "  f7 = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = NNP_VBZ(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "SUR4jl5cQfy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def IN_NNP(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'IN' and tagged[1][1] == 'NNP':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def IN_NNPCount(paragraphs):\n",
        "  f7 = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = IN_NNP(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "PTlxihUTUVgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lenLongestWord(paragraphs):\n",
        "  longestWord = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      longestWord.append(0)\n",
        "      continue\n",
        "    word_list = item.split()\n",
        "    # Find the longest word\n",
        "    #print(word_list)\n",
        "    longest_word = max(word_list, key = len)\n",
        "    #print(longest_word)\n",
        "    longestWord.append(len(longest_word))\n",
        "  return longestWord"
      ],
      "metadata": {
        "id": "FCadviSHUhPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WRBCount(paragraphs):\n",
        "  WRBCount = []\n",
        "  for row in range(len(paragraphs)):\n",
        "    count = 0\n",
        "    if(type(paragraphs[row]) == float):\n",
        "      WRBCount.append(0)\n",
        "      continue\n",
        "    for word,pos in nltk.pos_tag(nltk.word_tokenize(paragraphs[row])):\n",
        "          if (pos == 'WRB'):\n",
        "            count += 1\n",
        "            #print(word)\n",
        "    WRBCount.append(count)\n",
        "  return WRBCount"
      ],
      "metadata": {
        "id": "Mrs67QChUhUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WRB_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'WRB' and tagged[1][1] == 'POS':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "\n",
        "def WRB_POSCount(paragraphs):\n",
        "  f7 = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = WRB_POS(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "1eLokehRUhYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNCount(paragraphs):\n",
        "  NNCount = []\n",
        "  for row in range(len(paragraphs)):\n",
        "    count = 0\n",
        "    if(type(paragraphs[row]) == float):\n",
        "      NNCount.append(0)\n",
        "      continue\n",
        "    for word,pos in nltk.pos_tag(nltk.word_tokenize(paragraphs[row])):\n",
        "          if (pos == 'NN'):\n",
        "            count += 1\n",
        "    NNCount.append(count)\n",
        "  return NNCount"
      ],
      "metadata": {
        "id": "FoiE8M_2UVkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NN_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NN' and tagged[1][1] == 'POS':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def NN_POSCount(paragraphs):\n",
        "  f7 = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = NN_POS(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "Ih7ChhtQUVnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def startsWith5WH(paragraphs):\n",
        "  f = []\n",
        "  for item in paragraphs:\n",
        "    #print(item)\n",
        "    if(type(item) == float):\n",
        "      f.append(0)\n",
        "      continue\n",
        "    if item.lower().startswith(\"what\") or item.lower().startswith(\"where\") or item.lower().startswith(\"when\") or item.lower().startswith(\"which\") or item.lower().startswith(\"how\"):\n",
        "      f.append(1)\n",
        "    else:\n",
        "      f.append(0)\n",
        "  return f"
      ],
      "metadata": {
        "id": "a6f1VRAtUVrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def questionMark(paragraphs):\n",
        "  questionMark = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      questionMark.append(0)\n",
        "      continue\n",
        "    token = nltk.word_tokenize(item)\n",
        "    #print(token)\n",
        "    try:\n",
        "      token.index(\"?\")\n",
        "      questionMark.append(1)\n",
        "    except ValueError:\n",
        "      questionMark.append(0)\n",
        "  return questionMark"
      ],
      "metadata": {
        "id": "QXVdmeB8UVtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def this_these_NN_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NN' and (tagged[0][0].casefold() == \"this\".casefold() or tagged[0][0].casefold() == 'these'.casefold()) and tagged[1][1] == 'POS':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def this_these_NN_POSCount(paragraphs):\n",
        "  f7 = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = this_these_NN_POS(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "RuyElAagUVw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PRP_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'PRP' and tagged[1][1] == 'POS':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def PRP_POSCount(paragraphs):\n",
        "  f7 = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = PRP_POS(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "2Vv9R9nOUV0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FEATURE 21 : PRP count\n",
        "def PRPCount(paragraphs):\n",
        "  PRPCount = []\n",
        "  for row in range(len(paragraphs)):\n",
        "    count = 0\n",
        "    if(type(paragraphs[row]) == float):\n",
        "      PRPCount.append(0)\n",
        "      continue\n",
        "    for word,pos in nltk.pos_tag(nltk.word_tokenize(paragraphs[row])):\n",
        "          if (pos == 'PRP'):\n",
        "            count += 1\n",
        "    PRPCount.append(count)\n",
        "  return PRPCount"
      ],
      "metadata": {
        "id": "B_3_oLnSUV3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def VBZCount(paragraphs):\n",
        "  VBZCount = []\n",
        "  for row in range(len(paragraphs)):\n",
        "    count = 0\n",
        "    if(type(paragraphs[row]) == float):\n",
        "      VBZCount.append(0)\n",
        "      continue\n",
        "    for word,pos in nltk.pos_tag(nltk.word_tokenize(paragraphs[row])):\n",
        "          if (pos == 'VBZ'):\n",
        "            count += 1\n",
        "    VBZCount.append(count)\n",
        "  return VBZCount"
      ],
      "metadata": {
        "id": "NoyDsPoxUV6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_NNP_VBZ(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'NNP' and tagged[2][1] == 'VBZ':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def NNP_NNP_VBZCount(postText):\n",
        "  f7 = []\n",
        "  for item in postText:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.trigrams(Tokens))\n",
        "      cnt = NNP_NNP_VBZ(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "t7qU8B7dUV9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NN_IN(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NN' and tagged[1][1] == 'IN':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def NN_INCount(postText):\n",
        "  f7 = []\n",
        "  for item in postText:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = NN_IN(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "QS5Ax3U_UWAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NN_IN_NNP(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NN' and tagged[1][1] == 'IN' and tagged[2][1] == 'NNP':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def NN_IN_NNPCount(postText):\n",
        "  f7 = []\n",
        "  for item in postText:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.trigrams(Tokens))\n",
        "      cnt = NN_IN_NNP(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "NV86WlUxQf1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopWordsRatio(paragraphs):\n",
        "  ratioStopWords = []\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  for row in paragraphs:\n",
        "    if(type(row) == float):\n",
        "      ratioStopWords.append(0)\n",
        "      continue\n",
        "    tokens = row.split()\n",
        "    #print(tokens)\n",
        "    tokens_with_sw = [word for word in tokens if word in stop_words]\n",
        "    ratioStopWords.append(round(len(tokens_with_sw)/len(tokens),2))\n",
        "    #print(tokens_with_sw)\n",
        "  return ratioStopWords"
      ],
      "metadata": {
        "id": "fiwl8w50V483"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bigram_NNP(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' or tagged[1][1] == 'NNP':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def bigram_NNPCount(paragraphs):\n",
        "  f7 = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = bigram_NNP(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "qYXFQuy_V5Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PRP_VBP(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'PRP' and tagged[1][1] == 'VBP':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def PRP_VBPCount(paragraphs):\n",
        "  f7 = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = PRP_VBP(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "nhgwCipaV5E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WP_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'WP' and tagged[1][1] == 'POS':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def WP_POSCount(postText):\n",
        "  f7 = []\n",
        "  for item in postText:\n",
        "    if(type(item)== float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = WP_POS(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "hhYlZftYWJ3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def noWP(postText):\n",
        "  WPCount = []\n",
        "  for row in range(len(postText)):\n",
        "    if(type(postText[row]) == float):\n",
        "      WPCount.append(0)\n",
        "      continue\n",
        "    count = 0\n",
        "    for word,pos in nltk.pos_tag(nltk.word_tokenize(postText[row])):\n",
        "          if (pos == 'WP'):\n",
        "            count += 1\n",
        "    WPCount.append(count)\n",
        "  return WPCount"
      ],
      "metadata": {
        "id": "VR1vpViDWJ7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DT_count(s):\n",
        "  tagged = nltk.pos_tag(s)\n",
        "  cnt = 0\n",
        "  for item in tagged:\n",
        "    if item[1]=='DT':\n",
        "      # print(item[1])\n",
        "      cnt=cnt+1\n",
        "  return cnt\n",
        "\n",
        "def DT(Text):\n",
        "  f1 = []\n",
        "  for item in Text:\n",
        "    if(type(item)==float):\n",
        "      f1.append(0)\n",
        "      continue\n",
        "    s = nltk.word_tokenize(item)\n",
        "    cnt = DT_count(s)\n",
        "    f1.append(cnt)\n",
        "  return f1"
      ],
      "metadata": {
        "id": "OxCRVybaWykq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_IN(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'IN':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def NnpIn(Text):\n",
        "  f2 = []\n",
        "  for item in Text:\n",
        "    if(type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = NNP_IN(output)\n",
        "      f2.append(cnt)\n",
        "    else:\n",
        "      f2.append(0)\n",
        "  return f2"
      ],
      "metadata": {
        "id": "Kgr_WrBaWytL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def IN_NNP_NNP(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'IN' and tagged[1][1] == 'NNP' and tagged[0][1] == 'NNP':\n",
        "      cnt+=1\n",
        "      # print(tagged)\n",
        "  return cnt\n",
        "def In_Nnp_Nnp(Text):\n",
        "  f2 = []\n",
        "  for item in Text:\n",
        "    if( type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.trigrams(Tokens))  \n",
        "      cnt = NNP_IN(output)\n",
        "      f2.append(cnt)\n",
        "    else:\n",
        "      f2.append(0)\n",
        "  return f2"
      ],
      "metadata": {
        "id": "N1H248_0Wyw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def POS_count(s):\n",
        "  l = nltk.word_tokenize(s)\n",
        "  tagged = nltk.pos_tag(l)\n",
        "  cnt =0 \n",
        "  for item in tagged:\n",
        "    # print(item)\n",
        "    if item[1]=='POS':\n",
        "      # print(item)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "def POS(Text):\n",
        "  l = []\n",
        "  for item in Text:\n",
        "    if(type(item) == float ):\n",
        "      l.append(0)\n",
        "      continue\n",
        "    cnt = POS_count(item)\n",
        "    l.append(cnt)\n",
        "  return l"
      ],
      "metadata": {
        "id": "1DKeiK2hWy0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def IN_NN(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'IN' and tagged[1][1] == 'NN':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def In_Nn(text):\n",
        "  f2 = []\n",
        "  for item in text:\n",
        "    if(type(item) == str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = IN_NN(output)\n",
        "      f2.append(cnt)\n",
        "    else:\n",
        "      f2.append(0)\n",
        "  return f2"
      ],
      "metadata": {
        "id": "1oz3P3wQWy4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semi_delimeter_count(s):\n",
        "  l = nltk.word_tokenize(s)\n",
        "  cnt = 0\n",
        "  for item in l:\n",
        "    if item == ',':\n",
        "      # print(item)\n",
        "      cnt=cnt+1\n",
        "  return cnt \n",
        "def SemiDelimeter(text):\n",
        "  f4 = []\n",
        "  for item in text:\n",
        "    if(type(item)==float):\n",
        "      f4.append(0)\n",
        "      continue\n",
        "    cnt = semi_delimeter_count(item)\n",
        "    f4.append(cnt)\n",
        "  return f4"
      ],
      "metadata": {
        "id": "JQAClUP5Wy8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_NNS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'NNS':\n",
        "      cnt+=1\n",
        "      # print(tagged)\n",
        "  return cnt \n",
        "def Nnp_Nns(text):\n",
        "  f5 = []\n",
        "  for item in text:\n",
        "    if(type(item) == str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = NNP_NNS(output)\n",
        "      f5.append(cnt)\n",
        "    else:\n",
        "      f5.append(0)\n",
        "  return f5"
      ],
      "metadata": {
        "id": "CvNZ66mCWJ_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def IN_JJ(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'IN' and tagged[1][1] == 'JJ':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def In_Jj(text):\n",
        "  f6 = []\n",
        "  for item in text:\n",
        "    if(type(item) == str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = IN_JJ(output)\n",
        "      f6.append(cnt)\n",
        "    else:\n",
        "      f6.append(0)\n",
        "  return f6"
      ],
      "metadata": {
        "id": "Hi1dTMhxWKC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'POS':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def Nnp_Pos(text):\n",
        "  f7 = []\n",
        "  for item in text:\n",
        "    if type(item)==str and len(item.split())>1 :  \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = NNP_POS(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "CP6frE-hWKGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WDT(s):\n",
        "  l = nltk.word_tokenize(s)\n",
        "  output = nltk.pos_tag(l)\n",
        "  cnt = 0\n",
        "  for item in output:\n",
        "    if item[1] == 'WDT':\n",
        "      # print(item)\n",
        "      cnt=cnt+1\n",
        "  return cnt \n",
        "def Wdt(text):\n",
        "  f8 = []\n",
        "  for item in text:\n",
        "    if type(item)==float:\n",
        "      f8.append(0)\n",
        "      continue\n",
        "    cnt = WDT(item)\n",
        "    f8.append(cnt)\n",
        "  return f8"
      ],
      "metadata": {
        "id": "9A5ShvpMV5Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WDT_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'WDT' and tagged[1][1] == 'POS':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def Wdt_Pos(text):\n",
        "  f8 = []\n",
        "  for item in text:\n",
        "    if(type(item) == str and  len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = WDT_POS(output)\n",
        "      f8.append(cnt)\n",
        "    else:\n",
        "      f8.append(0)\n",
        "  return f8"
      ],
      "metadata": {
        "id": "aVcquS07V5Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NN_NN(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NN' and tagged[1][1] == 'NN':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "def Nn_Nn(Text):\n",
        "  f9 = []\n",
        "  for item in Text:\n",
        "    if(type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = NN_NN(output)\n",
        "      f9.append(cnt)\n",
        "    else:\n",
        "      f9.append(0)\n",
        "  return f9"
      ],
      "metadata": {
        "id": "dnhc-txWQf4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NN_NNP(s):\n",
        "  cnt = 0 \n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NN' and tagged[1][1] == 'NNP':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def Nn_Nnp(Text):\n",
        "  f10 = []\n",
        "  for item in Text:\n",
        "    if(type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt =NN_NNP(output)\n",
        "      f10.append(cnt)\n",
        "    else:\n",
        "      f10.append(0)\n",
        "  return f10"
      ],
      "metadata": {
        "id": "0n_xD3wUZ93q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_VBD(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'VBD':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "def Nnp_Vbd(PostText1):\n",
        "  f10 = []\n",
        "  for item in PostText1:\n",
        "    if( type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = NNP_VBD(output)\n",
        "      f10.append(cnt)\n",
        "    else:\n",
        "      f10.append(0)\n",
        "  return f10"
      ],
      "metadata": {
        "id": "kRss9xeNZ964"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RB_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'RB' and tagged[1][1] == 'POS':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def Rb_Pos(text):\n",
        "  f11 = []\n",
        "  for item in text:\n",
        "    if(type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = RB_POS(output)\n",
        "      f11.append(cnt)\n",
        "    else:\n",
        "      f11.append(0)\n",
        "  return f11"
      ],
      "metadata": {
        "id": "rQfCuhuhabpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DT_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'DT' and tagged[1][1] == 'POS':\n",
        "      cnt+=1\n",
        "      # print(tagged)\n",
        "  return cnt \n",
        "def Dt_Pos(text):\n",
        "  f12 = []\n",
        "  for item in text:\n",
        "    if( type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = DT_POS(output)\n",
        "      f12.append(cnt)\n",
        "    else:\n",
        "      f12.append(0)\n",
        "  return f12"
      ],
      "metadata": {
        "id": "H-OCwYo8absm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RB(s):\n",
        "  l = nltk.word_tokenize(s)\n",
        "  output = nltk.pos_tag(l)\n",
        "  cnt = 0\n",
        "  for item in output:\n",
        "    if item[1] == 'RB':\n",
        "      # print(item)\n",
        "      cnt=cnt+1\n",
        "  return cnt \n",
        "def Rb(postText1):\n",
        "  f12 = []\n",
        "  for item in postText1:\n",
        "    if type(item) == float:\n",
        "      f12.append(0)\n",
        "      continue\n",
        "    cnt = RB(item)\n",
        "    f12.append(cnt)\n",
        "  return f12"
      ],
      "metadata": {
        "id": "Q_YSS40yabwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_NNP_NNP(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'NNP' and tagged[2][1]=='NNP':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "def Nnp_Nnp_Nnp(postText1):\n",
        "  f13 = [] \n",
        "  for item in postText1:\n",
        "    if(type(item)==str and len(item.split())>2): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.trigrams(Tokens))\n",
        "      cnt = NNP_NNP_NNP(output)\n",
        "      f13.append(cnt)\n",
        "    else:\n",
        "      f13.append(0)\n",
        "  return f13"
      ],
      "metadata": {
        "id": "Nz5MC6i7Z997"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_NNP_NN(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'NNP' and tagged[2][1]=='NN':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "def Nnp_Nnp_Nn(postText1):\n",
        "  f14 = [] \n",
        "  for item in postText1:\n",
        "    if(type(item)==str and len(item.split())>2): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.trigrams(Tokens))\n",
        "      cnt = NNP_NNP_NN(output)\n",
        "      f14.append(cnt)\n",
        "    else:\n",
        "      f14.append(0)\n",
        "  return f14"
      ],
      "metadata": {
        "id": "Fhpg83b2Z-BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RBS(s):\n",
        "  l = nltk.word_tokenize(s)\n",
        "  output = nltk.pos_tag(l)\n",
        "  cnt = 0\n",
        "  for item in output:\n",
        "    if item[1] == 'RBS':\n",
        "      # print(item)\n",
        "      cnt=cnt+1\n",
        "  return cnt \n",
        "def Rbs(postText1):\n",
        "  f15 = []\n",
        "  for item in postText1:\n",
        "    if type(item)==float:\n",
        "      f15.append(0)\n",
        "      continue\n",
        "    cnt = RBS(item)\n",
        "    f15.append(cnt)\n",
        "  return f15"
      ],
      "metadata": {
        "id": "fZajq5k7bIkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def VBN(s):\n",
        "  l = nltk.word_tokenize(s)\n",
        "  output = nltk.pos_tag(l)\n",
        "  cnt = 0\n",
        "  for item in output:\n",
        "    if item[1] == 'VBN':\n",
        "      # print(item)\n",
        "      cnt=cnt+1\n",
        "  return cnt \n",
        "def Vbn(postText1):\n",
        "  f16 = []\n",
        "  for item in postText1:\n",
        "    if type(item)==float:\n",
        "      f16.append(0)\n",
        "      continue\n",
        "    cnt = VBN(item)\n",
        "    f16.append(cnt)\n",
        "  return f16"
      ],
      "metadata": {
        "id": "t_Ra4XpybInl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def VBN_IN(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'VBN' and tagged[1][1] == 'IN':\n",
        "      # print(item)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "def Vbn_In(postText1):\n",
        "  f17 = [] \n",
        "  for item in postText1:\n",
        "    if(type(item) == str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = VBN_IN(output)\n",
        "      f17.append(cnt)\n",
        "    else:\n",
        "      f17.append(0) \n",
        "  return f17"
      ],
      "metadata": {
        "id": "1rgrn6jLbIuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CD_NP_VB(s):\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'CD' and tagged[1][1] == 'NP' and tagged[2][1] == \"VB\":\n",
        "      # print(tagged)\n",
        "      return True\n",
        "  return False \n",
        "def Cd_Np_Vb(postText1):\n",
        "  f18 = [] \n",
        "  for item in postText1:\n",
        "    if(type(item)==str and len(item.split())>2): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.trigrams(Tokens))\n",
        "      flag = 1\n",
        "      if(CD_NP_VB(output)):\n",
        "        f18.append(1)\n",
        "        flag = 0\n",
        "        break\n",
        "      if flag:\n",
        "        f18.append(0)\n",
        "    else:\n",
        "      f18.append(0)\n",
        "  return f18"
      ],
      "metadata": {
        "id": "Duu5jvbbbIw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def JJ_NNP(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'JJ' and tagged[1][1]=='NNP':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def Jj_Nnp(text):\n",
        "  f19 = [] \n",
        "  for item in text:\n",
        "    if(type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = JJ_NNP(output)\n",
        "      f19.append(cnt)\n",
        "    else:\n",
        "      f19.append(0)\n",
        "  return f19"
      ],
      "metadata": {
        "id": "0bw_PaFLZ-Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_NN_NN(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1]=='NN' and tagged[2][1]=='NN':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def Nnp_Nn_Nn(postText1):\n",
        "  f19 = [] \n",
        "  for item in postText1:\n",
        "    if(type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.trigrams(Tokens))\n",
        "      cnt = NNP_NN_NN(output)\n",
        "      f19.append(cnt)\n",
        "    else:\n",
        "      f19.append(0)\n",
        "  return f19"
      ],
      "metadata": {
        "id": "72ZKMpEtbp4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DT_NN(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'DT' and tagged[1][1]=='NN':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "def Dt_Nn(postText1):\n",
        "  f19 = [] \n",
        "  for item in postText1:\n",
        "    if type(item)==float:\n",
        "      f19.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = DT_NN(output)\n",
        "      f19.append(cnt)\n",
        "    else:\n",
        "      f19.append(0)\n",
        "  return f19"
      ],
      "metadata": {
        "id": "GsKFtB-Fbp8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EX(s):\n",
        "  l = nltk.word_tokenize(s)\n",
        "  output = nltk.pos_tag(l)\n",
        "  for tagged in output:\n",
        "    if tagged[1] == 'EX':\n",
        "      # print(tagged)\n",
        "      return True\n",
        "  return False \n",
        "def Ex(postText1):\n",
        "  f19 = [] \n",
        "  for item in postText1:\n",
        "    if type(item)==float:\n",
        "      f19.append(0)\n",
        "      continue\n",
        "    if(EX(item)):\n",
        "       f19.append(1)\n",
        "    else:\n",
        "      f19.append(0)\n",
        "  return f19"
      ],
      "metadata": {
        "id": "jVEBDHvabp_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nNl6Ylnh5Pf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Readability of paragraphs using flesch_reading score **"
      ],
      "metadata": {
        "id": "ohQCFi1M2X4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "id": "MPUw0siWfBn7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "854aea4b-9658-4169-bfd5-ecad2d537b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.14.0 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textstat"
      ],
      "metadata": {
        "id": "o75_VbxafBrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_(s):\n",
        "  return textstat.(s)flesch_reading_ease"
      ],
      "metadata": {
        "id": "ElGmZweYfB1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_para(p):\n",
        "  l = []\n",
        "  for item in p:\n",
        "    if type(item)==float or  len(item)==0 or  type(item[0])!=str or item[0]==\"\":\n",
        "      l.append(0)\n",
        "      continue\n",
        "    # s = list_to_str(item)\n",
        "    val = read_(item)\n",
        "    l.append(val)\n",
        "  return l"
      ],
      "metadata": {
        "id": "2FsXHPoW1urw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fkyp6RTFZyNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/nlp/Clickbit_dataset/clickbait17-train-170331/wrong/Feature/paragraph.csv\")"
      ],
      "metadata": {
        "id": "QZ86rdinyZKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = dataset[\"targetParagraph\"]"
      ],
      "metadata": {
        "id": "2EboRbtyaXWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "Om7dQ1zv_r5B",
        "outputId": "41f06f69-93b5-4fe7-d9da-35a611863e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Paying for a 64GB phone only to discover that this is significantly reduced by system files and bloatware is the bane of many smartphone owner's lives.And the issue became so serious earlier this year that some Apple users even sued the company over it.But with the launch of iOS 9, Apple is hoping to address storage concerns by introducing a feature known as 'app thinning.'It has been explained on the watchOS Developer Library site and is aimed at developers looking to optimise their apps to work on iOS and the watchOS.It ensures apps use the lowest amount of storage space on a device by only downloading the parts it needs run on the particular handset it is being installed onto.It 'slices' the app into 'app variants' that only need to access the specific files on that specific handset.XperiaBlog recently spotted that the 8GB version of Sony's mid-range M4 Aqua has just 1.26GB of space for users.This means that firmware, pre-installed apps and Android software take up a staggering 84.25 per cent.Sony does let users increase storage space using a microSD card, but as XperiaBlog explained: 'Sony should never have launched an 8GB version of the Xperia M4 Aqua.'If you are thinking about purchasing this model, be aware of what you are buying into.'Previously, apps would need to be able to run on all handsets and account for the varying files, chipsets and power so contained sections that weren't always relevant to the phone it was being installed on.This made them larger than they needed to be.Under the new plans, when a phone is downloaded from the App Store, the app recognises which phone it is being installed onto and only pulls in the files and code it needs to work on that particular device.For iOS, sliced apps are supported on the latest iTunes and on devices running iOS 9.0 and later.In all other cases, the App Store will deliver the previous 'universal apps' to customers.The guidelines also discuss so-called 'on-demand resources.'This allows developers to omit features from an app until they are opened or requested by the user.The App Store hosts these resources on Apple servers and manages the downloads for the developer and user.This will also increase how quickly an app downloads.An example given by Apple is a game app that may divide resources into game levels and request the next level of resources only when the app anticipates the user has completed the previous level.Similarly, the app can request In-App Purchase resources only when the user buys a corresponding in-app purchase.Apple explained the operating system will then 'purge on-demand resources when they are no longer needed and disk space is low', removing them until they are needed again.And the whole iOS 9 software has been designed to be thinner during updates, namely from 4.6GB to 1.3GB, to free up space.This app thinning applies to third-party apps created by developers.Apple doesn't say if it will apply to the apps Apple pre-installed on devices, such as Stocks, Weather and Safari - but it is likely that it will in order to make iOS 9 smaller.As an example of storage space on Apple devices, a 64GB Apple iPhone 6 is typically left with 56GB of free space after pre-installed apps, system files and software is included.A drop of 8GB, leaving 87.5 per cent of storage free.By comparison, Samsung's 64GB S6 Edge has 53.42GB of available space, and of this 9GB is listed as system memory.Although this is a total drop of almost 11GB, it equates to 83 per cent of space free.By comparison, on a 32GB S6 MailOnline found 23.86GB of space was available, with 6.62GB attributed to system memory.This is a drop of just over 8GB and leaves 75 per cent free.Samsung said it, too, had addressed complaints about bloatware and storage space with its S6 range. Previous handsets, including the Samsung Galaxy S4 and Apple iPhone 5C typically ranged from between 54 per cent and 79 per cent of free space.Businessman 'killed his best friend when he crashed jet-powered dinghy into his £1million yacht while showing off' as his wife filmed them\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jAsN9l2vqsC",
        "outputId": "c20ad194-6b23-4011-ec3e-bd2f21b5f7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2459, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.DataFrame()\n",
        "df1[\"id\"]=dataset[\"id\"]\n",
        "title = paragraph"
      ],
      "metadata": {
        "id": "KYzU-h_HwrTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function call "
      ],
      "metadata": {
        "id": "hFDCfFU1jAQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1[\"paragraphs Number of NNP\"] = pd.DataFrame(noNNP(title))\n",
        "df1[\"paragraphs Number of tokens\"] = pd.DataFrame(noTokens(title))\n",
        "df1[\"paragraphs Word length of postText\"] = pd.DataFrame(wordLen(title))\n",
        "df1[\"paragraphs POS 2-gram NNP NNP\"] = pd.DataFrame(NNP_NNPCount(title))\n",
        "df1[\"paragraphs Whether post start with number\"] = pd.DataFrame(startsWithNumber(title))\n",
        "df1[\"paragraphs Average length of words in post\"] = pd.DataFrame(averageLen(title))\n",
        "df1[\"paragraphs Number of IN\"] = pd.DataFrame(INCount(title))\n",
        "df1[\"paragraphs POS 2-gram NNP VBZ\"] = pd.DataFrame(NNP_VBZCount(title))\n",
        "df1[\"paragraphs POS 2-gram IN NNP\"] = pd.DataFrame(IN_NNPCount(title))\n",
        "df1[\"paragraphs Length of the longest word in post text\"] = pd.DataFrame(lenLongestWord(title))\n",
        "df1[\"paragraphs Number of WRB\"] = pd.DataFrame(WRBCount(title))\n",
        "df1[\"paragraphs Count POS pattern WRB\"] = pd.DataFrame(WRB_POSCount(title))\n",
        "df1[\"paragraphs Number of NN\"] = pd.DataFrame(NNCount(title))\n",
        "df1[\"paragraphs Count POS pattern NN\"] = pd.DataFrame(NN_POSCount(title))\n",
        "df1[\"paragraphs Whether post text start with 5W1H\"] = pd.DataFrame(startsWith5WH(title))\n",
        "df1[\"paragraphs Whether exists QM\"] = pd.DataFrame(questionMark(title))\n",
        "df1[\"paragraphs Count POS pattern this/these NN\"] = pd.DataFrame(this_these_NN_POSCount(title))\n",
        "df1[\"paragraphs Count POS pattern PRP\"] = pd.DataFrame(PRP_POSCount(title))\n",
        "df1[\"paragraphs Number of PRP\"] = pd.DataFrame(PRPCount(title))\n",
        "df1[\"paragraphs Number of VBZ\"] = pd.DataFrame(VBZCount(title))\n",
        "df1[\"paragraphs POS 3-gram NNP NNP VBZ\"] = pd.DataFrame(NNP_NNP_VBZCount(title))\n",
        "df1[\"paragraphs POS 2-gram NN IN\"] = pd.DataFrame(NN_INCount(title))\n",
        "df1[\"paragraphs POS 3-gram NN IN NNP\"] = pd.DataFrame(NN_IN_NNPCount(title))\n",
        "df1[\"paragraphs Ratio of stop words in post text\"] = pd.DataFrame(stopWordsRatio(title))\n",
        "df1[\"paragraphs POS 2-gram NNP\"] = pd.DataFrame(bigram_NNPCount(title))\n",
        "df1[\"paragraphs POS 2-gram PRP VBP\"] = pd.DataFrame(PRP_VBPCount(title))\n",
        "df1[\"paragraphs Count POS pattern WP\"] = pd.DataFrame(WP_POSCount(title))\n",
        "df1[\"paragraphs Number of WP\"] = pd.DataFrame(noWP(title))\n"
      ],
      "metadata": {
        "id": "ZfXBFDdcwwhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1[\"paragraphs Number of DT\"] = pd.DataFrame(DT(title))\n",
        "df1[\"paragraphs Number of NNP IN\"] = pd.DataFrame(NnpIn(title))\n",
        "df1[\"paragraphs Number of IN NNP NNP\"] = pd.DataFrame(In_Nnp_Nnp(title))\n",
        "df1[\"paragraphs POS\"] = pd.DataFrame(POS(title))\n",
        "df1[\"paragraphs Number of IN NN\"] = pd.DataFrame(In_Nn(title))\n",
        "df1[\"paragraphs Number of Semi-Delimeter\"] = pd.DataFrame(SemiDelimeter(title))\n",
        "df1[\"paragraphs Number of NNP NNS\"] = pd.DataFrame(Nnp_Nns(title))\n",
        "df1[\"paragraphs Number of IN JJ\"] = pd.DataFrame(In_Jj(title))\n",
        "df1[\"paragraphs Number of NNP POS\"] = pd.DataFrame(Nnp_Pos(title))\n",
        "df1[\"paragraphs Number of WDT\"] = pd.DataFrame(Wdt(title))\n",
        "df1[\"paragraphs Number of WDT POS\"] = pd.DataFrame(Wdt_Pos(title))\n",
        "df1[\"paragraphs Number of NN NN\"] = pd.DataFrame(Nn_Nn(title))\n",
        "df1[\"paragraphs Number of NN NNP\"] = pd.DataFrame(Nn_Nnp(title))\n",
        "df1[\"paragraphs Number of NNP VBD\"] = pd.DataFrame(Nnp_Vbd(title))\n",
        "df1[\"paragraphs Number of RB POS\"] = pd.DataFrame(Rb_Pos(title))\n",
        "df1[\"paragraphs Number of DT POS\"] = pd.DataFrame(Dt_Pos(title))\n",
        "df1[\"paragraphs Number of Rb\"] = pd.DataFrame(Rb(title))\n",
        "df1[\"paragraphs Number of NNP NNP NNP\"] = pd.DataFrame(Nnp_Nnp_Nnp(title))\n",
        "df1[\"paragraphs Number of NNP NNP NN\"] = pd.DataFrame(Nnp_Nnp_Nn(title))\n",
        "df1[\"paragraphs Number of Rbs\"] = pd.DataFrame(Rbs(title))\n",
        "df1[\"paragraphs Number of VBN\"] = pd.DataFrame(Vbn(title))\n",
        "df1[\"paragraphs Number of VBN IN\"] = pd.DataFrame(Vbn_In(title))\n",
        "df1[\"paragraphs Number of CD NP VB\"] = pd.DataFrame(Cd_Np_Vb(title))\n",
        "df1[\"paragraphs Number of JJ NNP\"] = pd.DataFrame(Jj_Nnp(title))\n",
        "df1[\"paragraphs Number of NNP NN NN\"] = pd.DataFrame(Nnp_Nn_Nn(title))\n",
        "df1[\"paragraphs Number of DT NN\"] = pd.DataFrame(Dt_Nn(title))\n",
        "df1[\"paragraphs Existance of EX\"] = pd.DataFrame(Ex(title))\n",
        "df1[\"paragraph Readability\"] = pd.DataFrame(read_para(title))\n",
        "\n"
      ],
      "metadata": {
        "id": "5A7Li9PN0rPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting in csv"
      ],
      "metadata": {
        "id": "HB_NTljUjHvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.to_csv(\"/content/drive/MyDrive/nlp/Clickbit_dataset/clickbait17-validation-170630/feature/csv/paraFeature_60.csv\",index=False)"
      ],
      "metadata": {
        "id": "Dq1lnX0zwfSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv(\"/content/drive/MyDrive/nlp/Clickbit_dataset/clickbait17-validation-170630/feature/csv/paraFeature_60.csv\")"
      ],
      "metadata": {
        "id": "JbU2sApyjXT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "id": "NHVGBP6tjjDQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}