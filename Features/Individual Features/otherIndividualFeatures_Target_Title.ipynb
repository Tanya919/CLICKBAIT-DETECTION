{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEg3rw99PiHX",
        "outputId": "ad131ad7-024f-43a8-8f09-e13d821b68d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from statistics import variance\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "jx21-oZyQZ_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2srLoA7QfNV",
        "outputId": "295f093c-a5e7-4d49-8447-28fe65e3c916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kizoawf1QfR1",
        "outputId": "05765e63-22c8-415a-9c5c-3fea64e7b8fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aE9ZIYEQfXf",
        "outputId": "7b8383a4-9914-4403-ed14-0238f4b521ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculation of n-gram POS of target title \n",
        "#calculation of readability of target title"
      ],
      "metadata": {
        "id": "G6L39xU41x9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def noNNP(postText):\n",
        "  nnpCount = []\n",
        "  for row in range(len(postText)):\n",
        "    if type(postText[row])==float:\n",
        "      nnpCount.append(0)\n",
        "      continue\n",
        "    count = 0\n",
        "    for word,pos in nltk.pos_tag(nltk.word_tokenize(postText[row])):\n",
        "          if (pos == 'NNP'):\n",
        "            count += 1\n",
        "    nnpCount.append(count)\n",
        "  return nnpCount"
      ],
      "metadata": {
        "id": "FLODQg7HQfam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def noTokens(paragraphs):\n",
        "  tokenCount = []\n",
        "  for row in range(len(paragraphs)):\n",
        "    if type(paragraphs[row])==float:\n",
        "      tokenCount.append(0)\n",
        "      continue\n",
        "    tokens = nltk.word_tokenize(paragraphs[row])\n",
        "    tokenCount.append(len(tokens))\n",
        "    #print(tokens)\n",
        "  #print(tokenCount)\n",
        "  return tokenCount"
      ],
      "metadata": {
        "id": "ZxdhyDzLQfdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wordLen(paragraphs):\n",
        "  wordlen = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float or len(item) == 0):\n",
        "      wordlen.append(0)\n",
        "      continue\n",
        "    token = item.split()\n",
        "    wordlength = sum(len(word) for word in token)\n",
        "    #print(token)\n",
        "    #print(wordlength)\n",
        "    wordlen.append(len(wordlength))\n",
        "  return wordlen"
      ],
      "metadata": {
        "id": "-oYkPyrXQfhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_NNP(phrase):\n",
        "  cnt = 0\n",
        "  for item in phrase:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'NNP':\n",
        "      cnt += 1\n",
        "      #print(tagged)\n",
        "  return cnt\n",
        "\n",
        "#feature 5 - POS 2 GRAM NNP\n",
        "def NNP_NNPCount(paragraphs):\n",
        "  f = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item)== float):\n",
        "      f.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      #print(output)\n",
        "      cnt = NNP_NNP(output)\n",
        "      f.append(cnt)\n",
        "    else:\n",
        "      f.append(0)\n",
        "  return f"
      ],
      "metadata": {
        "id": "yq2l4sD1QfkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def startsWithNumber(paragraphs):\n",
        "  startNo = []\n",
        "  for row in range(len(paragraphs)):\n",
        "    if(type(paragraphs[row]) == float):\n",
        "      startNo.append(0)\n",
        "      continue\n",
        "    if(paragraphs[row][0].isdigit()):\n",
        "      startNo.append(1)\n",
        "    else:\n",
        "      startNo.append(0)\n",
        "  return startNo"
      ],
      "metadata": {
        "id": "tTMjSFFmQfnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def averageLen(paragraphs):\n",
        "  averageLenWords = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item)== float):\n",
        "      averageLenWords.append(0)\n",
        "      continue\n",
        "    words = item.split()\n",
        "    average = round(sum(len(word) for word in words) / len(words),2)\n",
        "    averageLenWords.append(average)\n",
        "  return averageLenWords"
      ],
      "metadata": {
        "id": "sHBmlZ1MQfrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def INCount(paragraphs):\n",
        "  INCount = []\n",
        "  for row in range(len(paragraphs)):\n",
        "    count = 0\n",
        "    if(type(paragraphs[row]) == float):\n",
        "      INCount.append(0)\n",
        "      continue \n",
        "    for word,pos in nltk.pos_tag(nltk.word_tokenize(paragraphs[row])):\n",
        "          if (pos == 'IN'):\n",
        "            count += 1\n",
        "    INCount.append(count)\n",
        "  return INCount"
      ],
      "metadata": {
        "id": "lb84zti6Qfwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_VBZ(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'VBZ':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def NNP_VBZCount(paragraphs):\n",
        "  f7 = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = NNP_VBZ(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "SUR4jl5cQfy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def IN_NNP(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'IN' and tagged[1][1] == 'NNP':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def IN_NNPCount(paragraphs):\n",
        "  f7 = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = IN_NNP(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "PTlxihUTUVgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lenLongestWord(paragraphs):\n",
        "  longestWord = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      longestWord.append(0)\n",
        "      continue\n",
        "    word_list = item.split()\n",
        "    # Find the longest word\n",
        "    #print(word_list)\n",
        "    longest_word = max(word_list, key = len)\n",
        "    #print(longest_word)\n",
        "    longestWord.append(len(longest_word))\n",
        "  return longestWord"
      ],
      "metadata": {
        "id": "FCadviSHUhPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WRBCount(paragraphs):\n",
        "  WRBCount = []\n",
        "  for row in range(len(paragraphs)):\n",
        "    count = 0\n",
        "    if(type(paragraphs[row]) == float):\n",
        "      WRBCount.append(0)\n",
        "      continue\n",
        "    for word,pos in nltk.pos_tag(nltk.word_tokenize(paragraphs[row])):\n",
        "          if (pos == 'WRB'):\n",
        "            count += 1\n",
        "            #print(word)\n",
        "    WRBCount.append(count)\n",
        "  return WRBCount"
      ],
      "metadata": {
        "id": "Mrs67QChUhUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WRB_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'WRB' and tagged[1][1] == 'POS':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "\n",
        "def WRB_POSCount(paragraphs):\n",
        "  f7 = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = WRB_POS(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "1eLokehRUhYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNCount(paragraphs):\n",
        "  NNCount = []\n",
        "  for row in range(len(paragraphs)):\n",
        "    count = 0\n",
        "    if(type(paragraphs[row]) == float):\n",
        "      NNCount.append(0)\n",
        "      continue\n",
        "    for word,pos in nltk.pos_tag(nltk.word_tokenize(paragraphs[row])):\n",
        "          if (pos == 'NN'):\n",
        "            count += 1\n",
        "    NNCount.append(count)\n",
        "  return NNCount"
      ],
      "metadata": {
        "id": "FoiE8M_2UVkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NN_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NN' and tagged[1][1] == 'POS':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def NN_POSCount(paragraphs):\n",
        "  f7 = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = NN_POS(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "Ih7ChhtQUVnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def startsWith5WH(paragraphs):\n",
        "  f = []\n",
        "  for item in paragraphs:\n",
        "    #print(item)\n",
        "    if(type(item) == float):\n",
        "      f.append(0)\n",
        "      continue\n",
        "    if item.lower().startswith(\"what\") or item.lower().startswith(\"where\") or item.lower().startswith(\"when\") or item.lower().startswith(\"which\") or item.lower().startswith(\"how\"):\n",
        "      f.append(1)\n",
        "    else:\n",
        "      f.append(0)\n",
        "  return f"
      ],
      "metadata": {
        "id": "a6f1VRAtUVrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def questionMark(paragraphs):\n",
        "  questionMark = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      questionMark.append(0)\n",
        "      continue\n",
        "    token = nltk.word_tokenize(item)\n",
        "    #print(token)\n",
        "    try:\n",
        "      token.index(\"?\")\n",
        "      questionMark.append(1)\n",
        "    except ValueError:\n",
        "      questionMark.append(0)\n",
        "  return questionMark"
      ],
      "metadata": {
        "id": "QXVdmeB8UVtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def this_these_NN_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NN' and (tagged[0][0].casefold() == \"this\".casefold() or tagged[0][0].casefold() == 'these'.casefold()) and tagged[1][1] == 'POS':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def this_these_NN_POSCount(paragraphs):\n",
        "  f7 = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = this_these_NN_POS(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "RuyElAagUVw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PRP_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'PRP' and tagged[1][1] == 'POS':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def PRP_POSCount(paragraphs):\n",
        "  f7 = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = PRP_POS(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "2Vv9R9nOUV0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FEATURE 21 : PRP count\n",
        "def PRPCount(paragraphs):\n",
        "  PRPCount = []\n",
        "  for row in range(len(paragraphs)):\n",
        "    count = 0\n",
        "    if(type(paragraphs[row]) == float):\n",
        "      PRPCount.append(0)\n",
        "      continue\n",
        "    for word,pos in nltk.pos_tag(nltk.word_tokenize(paragraphs[row])):\n",
        "          if (pos == 'PRP'):\n",
        "            count += 1\n",
        "    PRPCount.append(count)\n",
        "  return PRPCount"
      ],
      "metadata": {
        "id": "B_3_oLnSUV3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def VBZCount(paragraphs):\n",
        "  VBZCount = []\n",
        "  for row in range(len(paragraphs)):\n",
        "    count = 0\n",
        "    if(type(paragraphs[row]) == float):\n",
        "      VBZCount.append(0)\n",
        "      continue\n",
        "    for word,pos in nltk.pos_tag(nltk.word_tokenize(paragraphs[row])):\n",
        "          if (pos == 'VBZ'):\n",
        "            count += 1\n",
        "    VBZCount.append(count)\n",
        "  return VBZCount"
      ],
      "metadata": {
        "id": "NoyDsPoxUV6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_NNP_VBZ(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'NNP' and tagged[2][1] == 'VBZ':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def NNP_NNP_VBZCount(postText):\n",
        "  f7 = []\n",
        "  for item in postText:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.trigrams(Tokens))\n",
        "      cnt = NNP_NNP_VBZ(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "t7qU8B7dUV9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NN_IN(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NN' and tagged[1][1] == 'IN':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def NN_INCount(postText):\n",
        "  f7 = []\n",
        "  for item in postText:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = NN_IN(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "QS5Ax3U_UWAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NN_IN_NNP(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NN' and tagged[1][1] == 'IN' and tagged[2][1] == 'NNP':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def NN_IN_NNPCount(postText):\n",
        "  f7 = []\n",
        "  for item in postText:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.trigrams(Tokens))\n",
        "      cnt = NN_IN_NNP(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "NV86WlUxQf1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopWordsRatio(paragraphs):\n",
        "  ratioStopWords = []\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  for row in paragraphs:\n",
        "    if(type(row) == float):\n",
        "      ratioStopWords.append(0)\n",
        "      continue\n",
        "    tokens = row.split()\n",
        "    #print(tokens)\n",
        "    tokens_with_sw = [word for word in tokens if word in stop_words]\n",
        "    ratioStopWords.append(round(len(tokens_with_sw)/len(tokens),2))\n",
        "    #print(tokens_with_sw)\n",
        "  return ratioStopWords"
      ],
      "metadata": {
        "id": "fiwl8w50V483"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bigram_NNP(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' or tagged[1][1] == 'NNP':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def bigram_NNPCount(paragraphs):\n",
        "  f7 = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = bigram_NNP(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "qYXFQuy_V5Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PRP_VBP(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'PRP' and tagged[1][1] == 'VBP':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def PRP_VBPCount(paragraphs):\n",
        "  f7 = []\n",
        "  for item in paragraphs:\n",
        "    if(type(item) == float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = PRP_VBP(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "nhgwCipaV5E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WP_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'WP' and tagged[1][1] == 'POS':\n",
        "      #print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "\n",
        "def WP_POSCount(postText):\n",
        "  f7 = []\n",
        "  for item in postText:\n",
        "    if(type(item)== float):\n",
        "      f7.append(0)\n",
        "      continue\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = WP_POS(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "hhYlZftYWJ3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def noWP(postText):\n",
        "  WPCount = []\n",
        "  for row in range(len(postText)):\n",
        "    if(type(postText[row]) == float):\n",
        "      WPCount.append(0)\n",
        "      continue\n",
        "    count = 0\n",
        "    for word,pos in nltk.pos_tag(nltk.word_tokenize(postText[row])):\n",
        "          if (pos == 'WP'):\n",
        "            count += 1\n",
        "    WPCount.append(count)\n",
        "  return WPCount"
      ],
      "metadata": {
        "id": "VR1vpViDWJ7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DT_count(s):\n",
        "  tagged = nltk.pos_tag(s)\n",
        "  cnt = 0\n",
        "  for item in tagged:\n",
        "    if item[1]=='DT':\n",
        "      # print(item[1])\n",
        "      cnt=cnt+1\n",
        "  return cnt\n",
        "\n",
        "def DT(Text):\n",
        "  f1 = []\n",
        "  for item in Text:\n",
        "    if(type(item)==float):\n",
        "      f1.append(0)\n",
        "      continue\n",
        "    s = nltk.word_tokenize(item)\n",
        "    cnt = DT_count(s)\n",
        "    f1.append(cnt)\n",
        "  return f1"
      ],
      "metadata": {
        "id": "OxCRVybaWykq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_IN(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'IN':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def NnpIn(Text):\n",
        "  f2 = []\n",
        "  for item in Text:\n",
        "    if(type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = NNP_IN(output)\n",
        "      f2.append(cnt)\n",
        "    else:\n",
        "      f2.append(0)\n",
        "  return f2"
      ],
      "metadata": {
        "id": "Kgr_WrBaWytL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def IN_NNP_NNP(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'IN' and tagged[1][1] == 'NNP' and tagged[0][1] == 'NNP':\n",
        "      cnt+=1\n",
        "      # print(tagged)\n",
        "  return cnt\n",
        "def In_Nnp_Nnp(Text):\n",
        "  f2 = []\n",
        "  for item in Text:\n",
        "    if( type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.trigrams(Tokens))  \n",
        "      cnt = NNP_IN(output)\n",
        "      f2.append(cnt)\n",
        "    else:\n",
        "      f2.append(0)\n",
        "  return f2"
      ],
      "metadata": {
        "id": "N1H248_0Wyw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def POS_count(s):\n",
        "  l = nltk.word_tokenize(s)\n",
        "  tagged = nltk.pos_tag(l)\n",
        "  cnt =0 \n",
        "  for item in tagged:\n",
        "    # print(item)\n",
        "    if item[1]=='POS':\n",
        "      # print(item)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "def POS(Text):\n",
        "  l = []\n",
        "  for item in Text:\n",
        "    if(type(item) == float ):\n",
        "      l.append(0)\n",
        "      continue\n",
        "    cnt = POS_count(item)\n",
        "    l.append(cnt)\n",
        "  return l"
      ],
      "metadata": {
        "id": "1DKeiK2hWy0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def IN_NN(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'IN' and tagged[1][1] == 'NN':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def In_Nn(text):\n",
        "  f2 = []\n",
        "  for item in text:\n",
        "    if(type(item) == str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = IN_NN(output)\n",
        "      f2.append(cnt)\n",
        "    else:\n",
        "      f2.append(0)\n",
        "  return f2"
      ],
      "metadata": {
        "id": "1oz3P3wQWy4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semi_delimeter_count(s):\n",
        "  l = nltk.word_tokenize(s)\n",
        "  cnt = 0\n",
        "  for item in l:\n",
        "    if item == ',':\n",
        "      # print(item)\n",
        "      cnt=cnt+1\n",
        "  return cnt \n",
        "def SemiDelimeter(text):\n",
        "  f4 = []\n",
        "  for item in text:\n",
        "    if(type(item)==float):\n",
        "      f4.append(0)\n",
        "      continue\n",
        "    cnt = semi_delimeter_count(item)\n",
        "    f4.append(cnt)\n",
        "  return f4"
      ],
      "metadata": {
        "id": "JQAClUP5Wy8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_NNS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'NNS':\n",
        "      cnt+=1\n",
        "      # print(tagged)\n",
        "  return cnt \n",
        "def Nnp_Nns(text):\n",
        "  f5 = []\n",
        "  for item in text:\n",
        "    if(type(item) == str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = NNP_NNS(output)\n",
        "      f5.append(cnt)\n",
        "    else:\n",
        "      f5.append(0)\n",
        "  return f5"
      ],
      "metadata": {
        "id": "CvNZ66mCWJ_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def IN_JJ(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'IN' and tagged[1][1] == 'JJ':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def In_Jj(text):\n",
        "  f6 = []\n",
        "  for item in text:\n",
        "    if(type(item) == str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = IN_JJ(output)\n",
        "      f6.append(cnt)\n",
        "    else:\n",
        "      f6.append(0)\n",
        "  return f6"
      ],
      "metadata": {
        "id": "Hi1dTMhxWKC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'POS':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def Nnp_Pos(text):\n",
        "  f7 = []\n",
        "  for item in text:\n",
        "    if type(item)==str and len(item.split())>1 :  \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = NNP_POS(output)\n",
        "      f7.append(cnt)\n",
        "    else:\n",
        "      f7.append(0)\n",
        "  return f7"
      ],
      "metadata": {
        "id": "CP6frE-hWKGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WDT(s):\n",
        "  l = nltk.word_tokenize(s)\n",
        "  output = nltk.pos_tag(l)\n",
        "  cnt = 0\n",
        "  for item in output:\n",
        "    if item[1] == 'WDT':\n",
        "      # print(item)\n",
        "      cnt=cnt+1\n",
        "  return cnt \n",
        "def Wdt(text):\n",
        "  f8 = []\n",
        "  for item in text:\n",
        "    if type(item)==float:\n",
        "      f8.append(0)\n",
        "      continue\n",
        "    cnt = WDT(item)\n",
        "    f8.append(cnt)\n",
        "  return f8"
      ],
      "metadata": {
        "id": "9A5ShvpMV5Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WDT_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'WDT' and tagged[1][1] == 'POS':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def Wdt_Pos(text):\n",
        "  f8 = []\n",
        "  for item in text:\n",
        "    if(type(item) == str and  len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = WDT_POS(output)\n",
        "      f8.append(cnt)\n",
        "    else:\n",
        "      f8.append(0)\n",
        "  return f8"
      ],
      "metadata": {
        "id": "aVcquS07V5Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NN_NN(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NN' and tagged[1][1] == 'NN':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "def Nn_Nn(Text):\n",
        "  f9 = []\n",
        "  for item in Text:\n",
        "    if(type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = NN_NN(output)\n",
        "      f9.append(cnt)\n",
        "    else:\n",
        "      f9.append(0)\n",
        "  return f9"
      ],
      "metadata": {
        "id": "dnhc-txWQf4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NN_NNP(s):\n",
        "  cnt = 0 \n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NN' and tagged[1][1] == 'NNP':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def Nn_Nnp(Text):\n",
        "  f10 = []\n",
        "  for item in Text:\n",
        "    if(type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt =NN_NNP(output)\n",
        "      f10.append(cnt)\n",
        "    else:\n",
        "      f10.append(0)\n",
        "  return f10"
      ],
      "metadata": {
        "id": "0n_xD3wUZ93q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_VBD(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'VBD':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "def Nnp_Vbd(PostText1):\n",
        "  f10 = []\n",
        "  for item in PostText1:\n",
        "    if( type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = NNP_VBD(output)\n",
        "      f10.append(cnt)\n",
        "    else:\n",
        "      f10.append(0)\n",
        "  return f10"
      ],
      "metadata": {
        "id": "kRss9xeNZ964"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RB_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'RB' and tagged[1][1] == 'POS':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def Rb_Pos(text):\n",
        "  f11 = []\n",
        "  for item in text:\n",
        "    if(type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = RB_POS(output)\n",
        "      f11.append(cnt)\n",
        "    else:\n",
        "      f11.append(0)\n",
        "  return f11"
      ],
      "metadata": {
        "id": "rQfCuhuhabpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DT_POS(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'DT' and tagged[1][1] == 'POS':\n",
        "      cnt+=1\n",
        "      # print(tagged)\n",
        "  return cnt \n",
        "def Dt_Pos(text):\n",
        "  f12 = []\n",
        "  for item in text:\n",
        "    if( type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = DT_POS(output)\n",
        "      f12.append(cnt)\n",
        "    else:\n",
        "      f12.append(0)\n",
        "  return f12"
      ],
      "metadata": {
        "id": "H-OCwYo8absm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RB(s):\n",
        "  l = nltk.word_tokenize(s)\n",
        "  output = nltk.pos_tag(l)\n",
        "  cnt = 0\n",
        "  for item in output:\n",
        "    if item[1] == 'RB':\n",
        "      # print(item)\n",
        "      cnt=cnt+1\n",
        "  return cnt \n",
        "def Rb(postText1):\n",
        "  f12 = []\n",
        "  for item in postText1:\n",
        "    if type(item) == float:\n",
        "      f12.append(0)\n",
        "      continue\n",
        "    cnt = RB(item)\n",
        "    f12.append(cnt)\n",
        "  return f12"
      ],
      "metadata": {
        "id": "Q_YSS40yabwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_NNP_NNP(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'NNP' and tagged[2][1]=='NNP':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "def Nnp_Nnp_Nnp(postText1):\n",
        "  f13 = [] \n",
        "  for item in postText1:\n",
        "    if(type(item)==str and len(item.split())>2): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.trigrams(Tokens))\n",
        "      cnt = NNP_NNP_NNP(output)\n",
        "      f13.append(cnt)\n",
        "    else:\n",
        "      f13.append(0)\n",
        "  return f13"
      ],
      "metadata": {
        "id": "Nz5MC6i7Z997"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_NNP_NN(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1] == 'NNP' and tagged[2][1]=='NN':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "def Nnp_Nnp_Nn(postText1):\n",
        "  f14 = [] \n",
        "  for item in postText1:\n",
        "    if(type(item)==str and len(item.split())>2): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.trigrams(Tokens))\n",
        "      cnt = NNP_NNP_NN(output)\n",
        "      f14.append(cnt)\n",
        "    else:\n",
        "      f14.append(0)\n",
        "  return f14"
      ],
      "metadata": {
        "id": "Fhpg83b2Z-BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RBS(s):\n",
        "  l = nltk.word_tokenize(s)\n",
        "  output = nltk.pos_tag(l)\n",
        "  cnt = 0\n",
        "  for item in output:\n",
        "    if item[1] == 'RBS':\n",
        "      # print(item)\n",
        "      cnt=cnt+1\n",
        "  return cnt \n",
        "def Rbs(postText1):\n",
        "  f15 = []\n",
        "  for item in postText1:\n",
        "    if type(item)==float:\n",
        "      f15.append(0)\n",
        "      continue\n",
        "    cnt = RBS(item)\n",
        "    f15.append(cnt)\n",
        "  return f15"
      ],
      "metadata": {
        "id": "fZajq5k7bIkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def VBN(s):\n",
        "  l = nltk.word_tokenize(s)\n",
        "  output = nltk.pos_tag(l)\n",
        "  cnt = 0\n",
        "  for item in output:\n",
        "    if item[1] == 'VBN':\n",
        "      # print(item)\n",
        "      cnt=cnt+1\n",
        "  return cnt \n",
        "def Vbn(postText1):\n",
        "  f16 = []\n",
        "  for item in postText1:\n",
        "    if type(item)==float:\n",
        "      f16.append(0)\n",
        "      continue\n",
        "    cnt = VBN(item)\n",
        "    f16.append(cnt)\n",
        "  return f16"
      ],
      "metadata": {
        "id": "t_Ra4XpybInl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def VBN_IN(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'VBN' and tagged[1][1] == 'IN':\n",
        "      # print(item)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "def Vbn_In(postText1):\n",
        "  f17 = [] \n",
        "  for item in postText1:\n",
        "    if(type(item) == str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = VBN_IN(output)\n",
        "      f17.append(cnt)\n",
        "    else:\n",
        "      f17.append(0) \n",
        "  return f17"
      ],
      "metadata": {
        "id": "1rgrn6jLbIuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CD_NP_VB(s):\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'CD' and tagged[1][1] == 'NP' and tagged[2][1] == \"VB\":\n",
        "      # print(tagged)\n",
        "      return True\n",
        "  return False \n",
        "def Cd_Np_Vb(postText1):\n",
        "  f18 = [] \n",
        "  for item in postText1:\n",
        "    if(type(item)==str and len(item.split())>2): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.trigrams(Tokens))\n",
        "      flag = 1\n",
        "      if(CD_NP_VB(output)):\n",
        "        f18.append(1)\n",
        "        flag = 0\n",
        "        break\n",
        "      if flag:\n",
        "        f18.append(0)\n",
        "    else:\n",
        "      f18.append(0)\n",
        "  return f18"
      ],
      "metadata": {
        "id": "Duu5jvbbbIw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def JJ_NNP(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'JJ' and tagged[1][1]=='NNP':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def Jj_Nnp(text):\n",
        "  f19 = [] \n",
        "  for item in text:\n",
        "    if(type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = JJ_NNP(output)\n",
        "      f19.append(cnt)\n",
        "    else:\n",
        "      f19.append(0)\n",
        "  return f19"
      ],
      "metadata": {
        "id": "0bw_PaFLZ-Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NNP_NN_NN(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'NNP' and tagged[1][1]=='NN' and tagged[2][1]=='NN':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt \n",
        "def Nnp_Nn_Nn(postText1):\n",
        "  f19 = [] \n",
        "  for item in postText1:\n",
        "    if(type(item)==str and len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.trigrams(Tokens))\n",
        "      cnt = NNP_NN_NN(output)\n",
        "      f19.append(cnt)\n",
        "    else:\n",
        "      f19.append(0)\n",
        "  return f19"
      ],
      "metadata": {
        "id": "72ZKMpEtbp4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DT_NN(s):\n",
        "  cnt = 0\n",
        "  for item in s:\n",
        "    tagged = nltk.pos_tag(item)\n",
        "    if tagged[0][1] == 'DT' and tagged[1][1]=='NN':\n",
        "      # print(tagged)\n",
        "      cnt+=1\n",
        "  return cnt\n",
        "def Dt_Nn(postText1):\n",
        "  f19 = [] \n",
        "  for item in postText1:\n",
        "    if(len(item.split())>1): \n",
        "      Tokens = nltk.word_tokenize(item)\n",
        "      output = list(nltk.bigrams(Tokens))\n",
        "      cnt = DT_NN(output)\n",
        "      f19.append(cnt)\n",
        "    else:\n",
        "      f19.append(0)\n",
        "  return f19"
      ],
      "metadata": {
        "id": "GsKFtB-Fbp8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EX(s):\n",
        "  l = nltk.word_tokenize(s)\n",
        "  output = nltk.pos_tag(l)\n",
        "  for tagged in output:\n",
        "    if tagged[1] == 'EX':\n",
        "      # print(tagged)\n",
        "      return True\n",
        "  return False \n",
        "def Ex(postText1):\n",
        "  f19 = [] \n",
        "  for item in postText1:\n",
        "    if(EX(item)):\n",
        "       f19.append(1)\n",
        "    else:\n",
        "      f19.append(0)\n",
        "  return f19"
      ],
      "metadata": {
        "id": "jVEBDHvabp_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/nlp/Clickbit_dataset/clickbait17-validation-170630/Dataset/instances.csv\")"
      ],
      "metadata": {
        "id": "Btvz8Z0uavUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Readability of titles in terms of flesch_reading score "
      ],
      "metadata": {
        "id": "ohQCFi1M2X4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "id": "MPUw0siWfBn7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92055adf-6928-4b63-a555-55c6590ad7ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.14.0 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textstat"
      ],
      "metadata": {
        "id": "o75_VbxafBrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_(s):\n",
        "  return textstat.flesch_reading_ease(s)"
      ],
      "metadata": {
        "id": "ElGmZweYfB1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_single_title(s):\n",
        "  l = []\n",
        "  for item in s:\n",
        "    val = read_(item)\n",
        "    l.append(val)\n",
        "  return l"
      ],
      "metadata": {
        "id": "2xuU-N8S1uzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title = dataset[\"targetTitle\"]"
      ],
      "metadata": {
        "id": "ISUXQNy63zZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function call"
      ],
      "metadata": {
        "id": "zji9Cgv6g8sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.DataFrame()\n",
        "df1[\"id\"]=dataset[\"id\"]"
      ],
      "metadata": {
        "id": "KYzU-h_HwrTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1[\"Number of NNP\"] = pd.DataFrame(noNNP(title))\n",
        "df1[\"Number of tokens\"] = pd.DataFrame(noTokens(title))\n",
        "df1[\"Word length of postText\"] = pd.DataFrame(wordLen(title))\n",
        "df1[\"POS 2-gram NNP NNP\"] = pd.DataFrame(NNP_NNPCount(title))\n",
        "df1[\"Whether post start with number\"] = pd.DataFrame(startsWithNumber(title))\n",
        "df1[\"Average length of words in post\"] = pd.DataFrame(averageLen(title))\n",
        "df1[\"Number of IN\"] = pd.DataFrame(INCount(title))\n",
        "df1[\"POS 2-gram NNP VBZ\"] = pd.DataFrame(NNP_VBZCount(title))\n",
        "df1[\"POS 2-gram IN NNP\"] = pd.DataFrame(IN_NNPCount(title))\n",
        "df1[\"Length of the longest word in post text\"] = pd.DataFrame(lenLongestWord(title))\n",
        "df1[\"Number of WRB\"] = pd.DataFrame(WRBCount(title))\n",
        "df1[\"Count POS pattern WRB\"] = pd.DataFrame(WRB_POSCount(title))\n",
        "df1[\"Number of NN\"] = pd.DataFrame(NNCount(title))\n",
        "df1[\"Count POS pattern NN\"] = pd.DataFrame(NN_POSCount(title))\n",
        "df1[\"Whether post text start with 5W1H\"] = pd.DataFrame(startsWith5WH(title))\n",
        "df1[\"Whether exists QM\"] = pd.DataFrame(questionMark(title))\n",
        "df1[\"Count POS pattern this/these NN\"] = pd.DataFrame(this_these_NN_POSCount(title))\n",
        "df1[\"Count POS pattern PRP\"] = pd.DataFrame(PRP_POSCount(title))\n",
        "df1[\"Number of PRP\"] = pd.DataFrame(PRPCount(title))\n",
        "df1[\"Number of VBZ\"] = pd.DataFrame(VBZCount(title))\n",
        "df1[\"POS 3-gram NNP NNP VBZ\"] = pd.DataFrame(NNP_NNP_VBZCount(title))\n",
        "df1[\"POS 2-gram NN IN\"] = pd.DataFrame(NN_INCount(title))\n",
        "df1[\"POS 3-gram NN IN NNP\"] = pd.DataFrame(NN_IN_NNPCount(title))\n",
        "df1[\"Ratio of stop words in post text\"] = pd.DataFrame(stopWordsRatio(title))\n",
        "df1[\"POS 2-gram NNP\"] = pd.DataFrame(bigram_NNPCount(title))\n",
        "df1[\"POS 2-gram PRP VBP\"] = pd.DataFrame(PRP_VBPCount(title))\n",
        "df1[\"Count POS pattern WP\"] = pd.DataFrame(WP_POSCount(title))\n",
        "df1[\"Number of WP\"] = pd.DataFrame(noWP(title))\n"
      ],
      "metadata": {
        "id": "qqy0gwTJ1u5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1[\"Number of DT\"] = pd.DataFrame(DT(title))\n",
        "df1[\"Number of NNP IN\"] = pd.DataFrame(NnpIn(title))\n",
        "df1[\"Number of IN NNP NNP\"] = pd.DataFrame(In_Nnp_Nnp(title))\n",
        "df1[\"POS\"] = pd.DataFrame(POS(title))\n",
        "df1[\"Number of IN NN\"] = pd.DataFrame(In_Nn(title))\n",
        "df1[\"Number of Semi-Delimeter\"] = pd.DataFrame(SemiDelimeter(title))\n",
        "df1[\"Number of NNP NNS\"] = pd.DataFrame(Nnp_Nns(title))\n",
        "df1[\"Number of IN JJ\"] = pd.DataFrame(In_Jj(title))\n",
        "df1[\"Number of NNP POS\"] = pd.DataFrame(Nnp_Pos(title))\n",
        "df1[\"Number of WDT\"] = pd.DataFrame(Wdt(title))\n",
        "df1[\"Number of WDT POS\"] = pd.DataFrame(Wdt_Pos(title))\n",
        "df1[\"Number of NN NN\"] = pd.DataFrame(Nn_Nn(title))\n",
        "df1[\"Number of NN NNP\"] = pd.DataFrame(Nn_Nnp(title))\n",
        "df1[\"Number of NNP VBD\"] = pd.DataFrame(Nnp_Vbd(title))\n",
        "df1[\"Number of RB POS\"] = pd.DataFrame(Rb_Pos(title))\n",
        "df1[\"Number of DT POS\"] = pd.DataFrame(Dt_Pos(title))\n",
        "df1[\"Number of Rb\"] = pd.DataFrame(Rb(title))\n",
        "df1[\"Number of NNP NNP NNP\"] = pd.DataFrame(Nnp_Nnp_Nnp(title))\n",
        "df1[\"Number of NNP NNP NN\"] = pd.DataFrame(Nnp_Nnp_Nn(title))\n",
        "df1[\"Number of Rbs\"] = pd.DataFrame(Rbs(title))\n",
        "df1[\"Number of VBN\"] = pd.DataFrame(Vbn(title))\n",
        "df1[\"Number of VBN IN\"] = pd.DataFrame(Vbn_In(title))\n",
        "df1[\"Number of CD NP VB\"] = pd.DataFrame(Cd_Np_Vb(title))\n",
        "df1[\"Number of JJ NNP\"] = pd.DataFrame(Jj_Nnp(title))\n",
        "df1[\"Number of NNP NN NN\"] = pd.DataFrame(Nnp_Nn_Nn(title))\n",
        "df1[\"Number of DT NN\"] = pd.DataFrame(Dt_Nn(title))\n",
        "df1[\"Existance of EX\"] = pd.DataFrame(Ex(title))\n"
      ],
      "metadata": {
        "id": "E1zoLn2kofnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1[\"readability of title\"] = pd.DataFrame(read_single_title(title))"
      ],
      "metadata": {
        "id": "xwdAr89L4W-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save titles features in csv file "
      ],
      "metadata": {
        "id": "tPY3ZXcnhE7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.to_csv(\"/content/drive/MyDrive/nlp/Model/csv/TitleFeature.csv\", index=False)"
      ],
      "metadata": {
        "id": "2JEcByHc55YJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}